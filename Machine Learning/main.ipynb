{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "from colorthief import ColorThief\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file used for integrating shape predictor(eye tracker) and jaundice detection\n",
    "\n",
    "# Load the Dlib shape predictor\n",
    "shape_predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load your jaundice detection model\n",
    "jaundice_model = tf.keras.models.load_model(\"models/jaundice_detector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_face_eye_cropping(img, M = 36, N = 42):\n",
    "    \"\"\"\n",
    "    \n",
    "    This is the function to crop and save the eye for an input of a full face\n",
    "    \n",
    "    Input:\n",
    "    img = The input image loaded with dlib\n",
    "    M, N = The landmark region of intrest ((42,46) for left eye and (36,42) for right eye)\n",
    "    \n",
    "    Output:\n",
    "    eye = The cropped image of the input where only the eye is present in the size of (150,150)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining Pre-Trained Models\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    landmark_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    # Detecting the faces in the input\n",
    "    faces = face_detector(img, 1)\n",
    "    face = faces[0]\n",
    "    \n",
    "    # Detecting landmarks in the image\n",
    "    base_img = img.copy()\n",
    "    landmarks_tuple = []\n",
    "    landmarks = landmark_detector(img,face)\n",
    "\n",
    "    for i in range(0, 68):\n",
    "        x = landmarks.part(i).x\n",
    "        y = landmarks.part(i).y\n",
    "\n",
    "        landmarks_tuple.append((x, y))\n",
    "            \n",
    "        cv2.circle(base_img, (x, y), 2, (255, 255, 255), -1)\n",
    "    \n",
    "    # Creating a route between landmarks\n",
    "    routes = [i for i in range(M,N)] + [M]\n",
    "    route_coordinates = []\n",
    "    base_img = img.copy()\n",
    "\n",
    "    # Filling route_coordinates\n",
    "    for i in range(0, len(routes) - 1):\n",
    "        source_point = routes[i]\n",
    "        target_point = routes[i+1]\n",
    "    \n",
    "        source_coordinate = landmarks_tuple[source_point]\n",
    "        target_coordinate = landmarks_tuple[target_point]\n",
    "    \n",
    "        route_coordinates.append(source_coordinate)\n",
    "    \n",
    "        cv2.line(base_img, source_coordinate, target_coordinate, (255, 255, 255), 2)\n",
    "        \n",
    "    # Getting the eye bounding box using route_coordinates\n",
    "    x_coords = [coord[0] for coord in route_coordinates]\n",
    "    y_coords = [coord[1] for coord in route_coordinates]\n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "    # Add padding\n",
    "    padding = 10\n",
    "    x_min = max(0, x_min - padding)\n",
    "    y_min = max(0, y_min - padding)\n",
    "    x_max = min(img.shape[1], x_max + padding)\n",
    "    y_max = min(img.shape[0], y_max + padding)\n",
    "\n",
    "    # Crop and resize\n",
    "    eye_region = img[y_min:y_max, x_min:x_max]\n",
    "    eye = cv2.resize(eye_region, (150, 150))\n",
    "    \n",
    "    return eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom preprocessing layer\n",
    "\n",
    "class PreprocessingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(100, 100)):\n",
    "        super(PreprocessingLayer, self).__init__()\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs\n",
    "        image_input, color_data_input = inputs\n",
    "\n",
    "        # Preprocessing the image: resizing\n",
    "        processed_image = tf.image.resize(image_input, self.target_size)\n",
    "\n",
    "        # Preprocessing the color data: normalize to range [0, 1]\n",
    "        processed_color_data = color_data_input / 255.0\n",
    "\n",
    "        return processed_image, processed_color_data\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Output shapes for both inputs\n",
    "        image_shape, color_data_shape = input_shape\n",
    "        return (\n",
    "            (image_shape[0], self.target_size[0], self.target_size[1], image_shape[3]),\n",
    "            (color_data_shape[0], color_data_shape[1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ color_data_input    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ preprocessing_laye… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PreprocessingLaye…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)]    │            │ color_data_input… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">912,769</span> │ preprocessing_la… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ preprocessing_la… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ color_data_input    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ preprocessing_laye… │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mPreprocessingLaye…\u001b[0m │ \u001b[38;5;34m3\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)]    │            │ color_data_input… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │    \u001b[38;5;34m912,769\u001b[0m │ preprocessing_la… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ preprocessing_la… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">912,769</span> (3.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m912,769\u001b[0m (3.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">912,769</span> (3.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m912,769\u001b[0m (3.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the integrated model\n",
    "inputs = tf.keras.Input(shape=(None, None, 3), name='image_input')\n",
    "color_data_input = tf.keras.Input(shape=(3,), name='color_data_input')\n",
    "x_image, x_color = PreprocessingLayer()([inputs, color_data_input])  # Preprocess both inputs\n",
    "outputs = jaundice_model([x_image, x_color])\n",
    "integrated_model = tf.keras.Model([inputs, color_data_input], outputs)\n",
    "integrated_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f53afa08544f5887979dd82b2dfff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3a02d8827e454e8be1d4b3e1941b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uploader to check integrated model by uploading an image\n",
    "\n",
    "uploaded = widgets.FileUpload(accept=\"image/*\", multiple=True)\n",
    "display(uploaded)\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "image_size = 100  \n",
    "\n",
    "temp_dir = \"temp_files\"\n",
    "os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "def process_uploaded_files(change):\n",
    "    \"\"\"Proses file yang diunggah ketika terjadi perubahan di widget.\"\"\"\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "\n",
    "        for file_data in uploaded.value:\n",
    "            file_name = file_data['name']\n",
    "            file_content = file_data['content']\n",
    "            \n",
    "            print(f'User uploaded file \"{file_name}\" with length {len(file_content)} bytes')\n",
    "            try:\n",
    "\n",
    "                temp_file_path = os.path.join(temp_dir, file_name)\n",
    "                with open(temp_file_path, \"wb\") as temp_file:\n",
    "                    temp_file.write(file_content)\n",
    "\n",
    "  \n",
    "                file_bytes = np.frombuffer(file_content, np.uint8)\n",
    "                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "                # img = cv2.resize(img, (image_size, image_size))\n",
    "\n",
    "  \n",
    "                color_thief = ColorThief(temp_file_path) \n",
    "                dominant_color = color_thief.get_color(quality=1)\n",
    "                normalized_color = [float(c) / 255.0 for c in dominant_color]\n",
    "\n",
    "               \n",
    "                img = np.expand_dims(img, axis=0) / 255.0\n",
    "\n",
    "               \n",
    "                prediction = integrated_model.predict([img, np.array([normalized_color])])\n",
    "\n",
    "                if prediction[0][0] > 0.5:\n",
    "                    print(f\"The image {file_name} is classified as Normal Eye\")\n",
    "                else:\n",
    "                    print(f\"The image {file_name} is classified as Jaundiced Eye\")\n",
    "\n",
    "              \n",
    "                try:\n",
    "                   \n",
    "                    img_display = (img[0] * 255).astype(np.uint8) \n",
    "                    plt.imshow(cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB))  \n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error displaying image: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "            finally:\n",
    "              \n",
    "                if os.path.exists(temp_file_path):\n",
    "                    os.remove(temp_file_path)\n",
    "\n",
    "\n",
    "uploaded.observe(process_uploaded_files, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the integrated model to keras format\n",
    "integrated_model.save(\"integrated_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpe2n49j1e\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpe2n49j1e\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\muham\\AppData\\Local\\Temp\\tmpe2n49j1e'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='image_input'), TensorSpec(shape=(None, 3), dtype=tf.float32, name='color_data_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2324121529760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121780640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121784688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121785216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121790144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121791024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121794720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121815696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121819920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2324121819392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(integrated_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a file\n",
    "with open('final_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model object as a .pkl file\n",
    "with open('final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(integrated_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the architecture (structure) to a .json file\n",
    "model_json = integrated_model.to_json()  # Get the architecture as a JSON string\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open('final_model.json', 'w') as json_file:\n",
    "    json.dump(model_json, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
