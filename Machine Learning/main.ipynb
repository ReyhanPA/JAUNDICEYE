{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "from colorthief import ColorThief\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file used for integrating shape predictor(eye tracker) and jaundice detection\n",
    "\n",
    "# Load the Dlib shape predictor\n",
    "shape_predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your jaundice detection model\n",
    "jaundice_model = tf.keras.models.load_model(\"models/jaundice_detector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_face_eye_cropping(img, M = 36, N = 42):\n",
    "    \"\"\"\n",
    "    \n",
    "    This is the function to crop and save the eye for an input of a full face\n",
    "    \n",
    "    Input:\n",
    "    img = The input image loaded with dlib\n",
    "    M, N = The landmark region of intrest ((42,46) for left eye and (36,42) for right eye)\n",
    "    \n",
    "    Output:\n",
    "    eye = The cropped image of the input where only the eye is present in the size of (150,150)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining Pre-Trained Models\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    landmark_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    # Detecting the faces in the input\n",
    "    faces = face_detector(img, 1)\n",
    "    face = faces[0]\n",
    "    \n",
    "    # Detecting landmarks in the image\n",
    "    base_img = img.copy()\n",
    "    landmarks_tuple = []\n",
    "    landmarks = landmark_detector(img,face)\n",
    "\n",
    "    for i in range(0, 68):\n",
    "        x = landmarks.part(i).x\n",
    "        y = landmarks.part(i).y\n",
    "\n",
    "        landmarks_tuple.append((x, y))\n",
    "            \n",
    "        cv2.circle(base_img, (x, y), 2, (255, 255, 255), -1)\n",
    "    \n",
    "    # Creating a route between landmarks\n",
    "    routes = [i for i in range(M,N)] + [M]\n",
    "    route_coordinates = []\n",
    "    base_img = img.copy()\n",
    "\n",
    "    # Filling route_coordinates\n",
    "    for i in range(0, len(routes) - 1):\n",
    "        source_point = routes[i]\n",
    "        target_point = routes[i+1]\n",
    "    \n",
    "        source_coordinate = landmarks_tuple[source_point]\n",
    "        target_coordinate = landmarks_tuple[target_point]\n",
    "    \n",
    "        route_coordinates.append(source_coordinate)\n",
    "    \n",
    "        cv2.line(base_img, source_coordinate, target_coordinate, (255, 255, 255), 2)\n",
    "        \n",
    "    # Getting the eye bounding box using route_coordinates\n",
    "    x_coords = [coord[0] for coord in route_coordinates]\n",
    "    y_coords = [coord[1] for coord in route_coordinates]\n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "    # Add padding\n",
    "    padding = 10\n",
    "    x_min = max(0, x_min - padding)\n",
    "    y_min = max(0, y_min - padding)\n",
    "    x_max = min(img.shape[1], x_max + padding)\n",
    "    y_max = min(img.shape[0], y_max + padding)\n",
    "\n",
    "    # Crop and resize\n",
    "    eye_region = img[y_min:y_max, x_min:x_max]\n",
    "    eye = cv2.resize(eye_region, (150, 150))\n",
    "    \n",
    "    return eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom preprocessing layer\n",
    "@register_keras_serializable()\n",
    "class PreprocessingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(100, 100)):\n",
    "        super(PreprocessingLayer, self).__init__()\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs\n",
    "        image_input, color_data_input = inputs\n",
    "\n",
    "        # Preprocessing the image: resizing\n",
    "        processed_image = tf.image.resize(image_input, self.target_size)\n",
    "\n",
    "        # Preprocessing the color data: normalize to range [0, 1]\n",
    "        processed_color_data = color_data_input / 255.0\n",
    "\n",
    "        return processed_image, processed_color_data\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Output shapes for both inputs\n",
    "        image_shape, color_data_shape = input_shape\n",
    "        return (\n",
    "            (image_shape[0], self.target_size[0], self.target_size[1], image_shape[3]),\n",
    "            (color_data_shape[0], color_data_shape[1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " color_data_input (InputLayer)  [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " preprocessing_layer (Preproces  ((None, 100, 100, 3  0          ['image_input[0][0]',            \n",
      " singLayer)                     ),                                'color_data_input[0][0]']       \n",
      "                                 (None, 3))                                                       \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1)            912769      ['preprocessing_layer[0][0]',    \n",
      "                                                                  'preprocessing_layer[0][1]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 912,769\n",
      "Trainable params: 912,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the integrated model\n",
    "inputs = tf.keras.Input(shape=(None, None, 3), name='image_input')\n",
    "color_data_input = tf.keras.Input(shape=(3,), name='color_data_input')\n",
    "x_image, x_color = PreprocessingLayer()([inputs, color_data_input])  # Preprocess both inputs\n",
    "outputs = jaundice_model([x_image, x_color])\n",
    "integrated_model = tf.keras.Model([inputs, color_data_input], outputs)\n",
    "integrated_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95797a410a14c44be24cec0d87c64ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56182cc8c2e94e0ea164d357ffa57c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uploader to check integrated model by uploading an image\n",
    "\n",
    "uploaded = widgets.FileUpload(accept=\"image/*\", multiple=True)\n",
    "display(uploaded)\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "image_size = 100  \n",
    "\n",
    "temp_dir = \"temp_files\"\n",
    "os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "def process_uploaded_files(change):\n",
    "    \"\"\"Proses file yang diunggah ketika terjadi perubahan di widget.\"\"\"\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "\n",
    "        for file_data in uploaded.value:\n",
    "            file_name = file_data['name']\n",
    "            file_content = file_data['content']\n",
    "            \n",
    "            print(f'User uploaded file \"{file_name}\" with length {len(file_content)} bytes')\n",
    "            try:\n",
    "\n",
    "                temp_file_path = os.path.join(temp_dir, file_name)\n",
    "                with open(temp_file_path, \"wb\") as temp_file:\n",
    "                    temp_file.write(file_content)\n",
    "\n",
    "  \n",
    "                file_bytes = np.frombuffer(file_content, np.uint8)\n",
    "                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "                # img = cv2.resize(img, (image_size, image_size))\n",
    "\n",
    "  \n",
    "                color_thief = ColorThief(temp_file_path) \n",
    "                dominant_color = color_thief.get_color(quality=1)\n",
    "                normalized_color = [float(c) / 255.0 for c in dominant_color]\n",
    "\n",
    "               \n",
    "                img = np.expand_dims(img, axis=0) / 255.0\n",
    "\n",
    "               \n",
    "                prediction = integrated_model.predict([img, np.array([normalized_color])])\n",
    "\n",
    "                if prediction[0][0] > 0.5:\n",
    "                    print(f\"The image {file_name} is classified as Normal Eye\")\n",
    "                else:\n",
    "                    print(f\"The image {file_name} is classified as Jaundiced Eye\")\n",
    "\n",
    "              \n",
    "                try:\n",
    "                   \n",
    "                    img_display = (img[0] * 255).astype(np.uint8) \n",
    "                    plt.imshow(cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB))  \n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error displaying image: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "            finally:\n",
    "              \n",
    "                if os.path.exists(temp_file_path):\n",
    "                    os.remove(temp_file_path)\n",
    "\n",
    "\n",
    "uploaded.observe(process_uploaded_files, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\nLayer PreprocessingLayer has arguments ['target_size']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the integrated model to keras format\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mintegrated_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_model_v2.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\base_layer.py:786\u001b[0m, in \u001b[0;36mLayer.get_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;66;03m# Check that either the only argument in the `__init__` is  `self`,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;66;03m# or that `get_config` has been overridden:\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_default\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    787\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    788\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124m  Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_args\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;124m  in `__init__` and therefore must override `get_config()`.\u001b[39m\n\u001b[0;32m    791\u001b[0m \n\u001b[0;32m    792\u001b[0m \u001b[38;5;124m  Example:\u001b[39m\n\u001b[0;32m    793\u001b[0m \n\u001b[0;32m    794\u001b[0m \u001b[38;5;124m  class CustomLayer(keras.layers.Layer):\u001b[39m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;124m      def __init__(self, arg1, arg2):\u001b[39m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124m          super().__init__()\u001b[39m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124m          self.arg1 = arg1\u001b[39m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;124m          self.arg2 = arg2\u001b[39m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[38;5;124m      def get_config(self):\u001b[39m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124m          config = super().get_config()\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124m          config.update(\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg1,\u001b[39m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg2,\u001b[39m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124m          \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124m          return config\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    807\u001b[0m         )\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: \nLayer PreprocessingLayer has arguments ['target_size']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"
     ]
    }
   ],
   "source": [
    "# Save the integrated model to keras format\n",
    "integrated_model.save(\"final_model_v2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpdth74y8p\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpdth74y8p\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(integrated_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a file\n",
    "with open('final_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e5fc3bf3-3853-406f-8149-9f2441aaa17f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e5fc3bf3-3853-406f-8149-9f2441aaa17f/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model object as a .pkl file\n",
    "with open('final_model_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(integrated_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
