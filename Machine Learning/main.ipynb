{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dlib shape predictor\n",
    "shape_predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load your jaundice detection model\n",
    "jaundice_model = tf.keras.models.load_model(\"models/jaundice_detector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_face_eye_cropping(img, M = 36, N = 42):\n",
    "    \"\"\"\n",
    "    \n",
    "    This is the function to crop and save the eye for an input of a full face\n",
    "    \n",
    "    Input:\n",
    "    img = The input image loaded with dlib\n",
    "    M, N = The landmark region of intrest ((42,46) for left eye and (36,42) for right eye)\n",
    "    \n",
    "    Output:\n",
    "    eye = The cropped image of the input where only the eye is present in the size of (150,150)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining Pre-Trained Models\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    landmark_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    # Detecting the faces in the input\n",
    "    faces = face_detector(img, 1)\n",
    "    face = faces[0]\n",
    "    \n",
    "    # Detecting landmarks in the image\n",
    "    base_img = img.copy()\n",
    "    landmarks_tuple = []\n",
    "    landmarks = landmark_detector(img,face)\n",
    "\n",
    "    for i in range(0, 68):\n",
    "        x = landmarks.part(i).x\n",
    "        y = landmarks.part(i).y\n",
    "\n",
    "        landmarks_tuple.append((x, y))\n",
    "            \n",
    "        cv2.circle(base_img, (x, y), 2, (255, 255, 255), -1)\n",
    "    \n",
    "    # Creating a route between landmarks\n",
    "    routes = [i for i in range(M,N)] + [M]\n",
    "    route_coordinates = []\n",
    "    base_img = img.copy()\n",
    "\n",
    "    # Filling route_coordinates\n",
    "    for i in range(0, len(routes) - 1):\n",
    "        source_point = routes[i]\n",
    "        target_point = routes[i+1]\n",
    "    \n",
    "        source_coordinate = landmarks_tuple[source_point]\n",
    "        target_coordinate = landmarks_tuple[target_point]\n",
    "    \n",
    "        route_coordinates.append(source_coordinate)\n",
    "    \n",
    "        cv2.line(base_img, source_coordinate, target_coordinate, (255, 255, 255), 2)\n",
    "        \n",
    "    # Getting the eye bounding box using route_coordinates\n",
    "    x_coords = [coord[0] for coord in route_coordinates]\n",
    "    y_coords = [coord[1] for coord in route_coordinates]\n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "    # Add padding\n",
    "    padding = 10\n",
    "    x_min = max(0, x_min - padding)\n",
    "    y_min = max(0, y_min - padding)\n",
    "    x_max = min(img.shape[1], x_max + padding)\n",
    "    y_max = min(img.shape[0], y_max + padding)\n",
    "\n",
    "    # Crop and resize\n",
    "    eye_region = img[y_min:y_max, x_min:x_max]\n",
    "    eye = cv2.resize(eye_region, (150, 150))\n",
    "    \n",
    "    return eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(150, 150)):\n",
    "        super(PreprocessingLayer, self).__init__()\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Preprocessing step: resizing the image\n",
    "        x = tf.image.resize(inputs, self.target_size)  # Resize input image to the target size\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output shape is determined by the target size of the preprocessing layer\n",
    "        return (input_shape[0], self.target_size[0], self.target_size[1], input_shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_38\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_38\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ preprocessing_layer_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PreprocessingLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,494,561</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ preprocessing_layer_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPreprocessingLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │     \u001b[38;5;34m9,494,561\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,494,561</span> (36.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,494,561\u001b[0m (36.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,494,561</span> (36.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,494,561\u001b[0m (36.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the integrated model\n",
    "inputs = tf.keras.Input(shape=(None, None, 3))  # Adjust input shape as needed\n",
    "x = PreprocessingLayer()(inputs)\n",
    "outputs = jaundice_model(x)\n",
    "integrated_model = tf.keras.Model(inputs, outputs)\n",
    "integrated_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the integrated model\n",
    "# integrated_model.save(\"integrated_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpg55k11bp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\muham\\AppData\\Local\\Temp\\tmpg55k11bp\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\muham\\AppData\\Local\\Temp\\tmpg55k11bp'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='keras_tensor_187')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2773950786976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773950009184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773948941232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951302112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951313200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951310032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951314960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951483216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951485328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2773951488144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# # Convert the model to TFLite\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(integrated_model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the converted model to a file\n",
    "# with open('final_model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the model object as a .pkl file\n",
    "# with open('final_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(integrated_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save the architecture (structure) to a .json file\n",
    "# model_json = integrated_model.to_json()  # Get the architecture as a JSON string\n",
    "\n",
    "# # Write the JSON string to a file\n",
    "# with open('final_model.json', 'w') as json_file:\n",
    "#     json.dump(model_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model before evaluation (if not already compiled)\n",
    "integrated_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories for jaundiced and normal eye images\n",
    "BASE_DIR = \"./Data/\"\n",
    "jaundiced_dir = os.path.join(BASE_DIR, \"Jaundiced Eyes/\")\n",
    "normal_dir = os.path.join(BASE_DIR, \"Normal Eyes/\")\n",
    "\n",
    "# Load images from directories\n",
    "jaundiced_images = [os.path.join(jaundiced_dir, fname) for fname in os.listdir(jaundiced_dir)]\n",
    "normal_images = [os.path.join(normal_dir, fname) for fname in os.listdir(normal_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels (1 for jaundiced, 0 for normal)\n",
    "test_images = jaundiced_images + normal_images\n",
    "test_labels = [1] * len(jaundiced_images) + [0] * len(normal_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6884 - loss: 0.6198\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "# Preprocess test images: Resize and preprocess as you did for training\n",
    "def preprocess_eye(img_path):\n",
    "    img = image.load_img(img_path, target_size=(150, 150))  # Resize to match input shape\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize if necessary (depends on your training preprocessing)\n",
    "    return img_array\n",
    "\n",
    "test_images_preprocessed = []\n",
    "for img_path in test_images:\n",
    "    preprocessed_image = preprocess_eye(img_path)\n",
    "    test_images_preprocessed.append(preprocessed_image)\n",
    "\n",
    "# Convert the images to a numpy array\n",
    "test_images_preprocessed = np.array(test_images_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = integrated_model.evaluate(test_images_preprocessed, np.array(test_labels))\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6b8be83354fe6a61c7ffe9db0a5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6687d3088004f01b7e15a7f3c7cdd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload(accept=\"image/*\", multiple=True)\n",
    "display(uploader)\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "def file_predict(filename, file, out):\n",
    "    \"\"\" A function for creating the prediction and printing the output.\"\"\"\n",
    "    image = tf.keras.utils.load_img(file, target_size=(150, 150))\n",
    "    image = tf.keras.utils.img_to_array(image)\n",
    "    image = rescale_layer(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    prediction = integrated_model.predict(image, verbose=0)[0][0]\n",
    "    print(prediction)\n",
    "\n",
    "    with out:\n",
    "        if prediction > 0.5:\n",
    "            print(filename + \" is a jaundiced eye\")\n",
    "        else:\n",
    "            print(filename + \" is a normal eye\")\n",
    "\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\" A function for geting files from the widget and running the prediction.\"\"\"\n",
    "\n",
    "    items = change.new\n",
    "    for item in items: # Loop if there is more than one file uploaded\n",
    "        file_jpgdata = BytesIO(item.content)\n",
    "        file_predict(item.name, file_jpgdata, out)\n",
    "uploader.observe(on_upload_change, names='value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
